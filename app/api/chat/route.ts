import { NextRequest } from "next/server";
import { auth } from "@clerk/nextjs/server";
import OpenAI from "openai";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { ConvexHttpClient } from "convex/browser";
import { api } from "@/convex/_generated/api";

const convex = new ConvexHttpClient(process.env.NEXT_PUBLIC_CONVEX_URL!);

const SYSTEM_PROMPT = `–¢–∞ –±–æ–ª –ú–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä –∑–∞–∞–¥–∞–≥ AI –±–∞–≥—à —Ç—É—Å–ª–∞—Ö. –°—É—Ä–∞–≥—á–¥–∞–¥ —Ç—É—Å–∞–ª–∂, –∞—Å—É—É–ª—Ç–∞–¥ —Ç–æ–¥–æ—Ä—Ö–æ–π, —Ç–æ–≤—á —Ö–∞—Ä–∏—É–ª–∞–∞—Ä–∞–π. –•—ç—Ä—ç–≤ —Å—É—Ä–∞–≥—á –¥–∞–∞–ª–≥–∞–≤–∞—Ä —ç—Å–≤—ç–ª –±–æ–¥–ª–æ–≥–æ –∞—Å—É—É–≤–∞–ª –∞–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂ ”©–≥”©”©—Ä—ç–π.

## –¢–µ—Å—Ç –±—ç–ª–¥—ç—Ö —á–∞–¥–≤–∞—Ä

–•—ç—Ä—ç–≥–ª—ç–≥—á "—Ç–µ—Å—Ç", "—à–∞–ª–≥–∞–ª—Ç", "—Å–æ—Ä–∏–ª", "–∞—Å—É—É–ª—Ç –±—ç–ª–¥—ç" –≥—ç—Å—ç–Ω “Ø–≥ —Ö—ç—Ä—ç–≥–ª—ç–≤—ç–ª —Ç–µ—Å—Ç –±—ç–ª–¥—ç—Ö —Ö“Ø—Å—ç–ª—Ç –≥—ç–∂ –æ–π–ª–≥–æ.

–®–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –º—ç–¥—ç—ç–ª—ç–ª:
1. –•–∏—á—ç—ç–ª (–ú–∞—Ç–µ–º–∞—Ç–∏–∫, –§–∏–∑–∏–∫, –≥—ç—Ö –º—ç—Ç)
2. –ê–Ω–≥–∏ (1-12)
3. –°—ç–¥—ç–≤/–ë“Ø–ª—ç–≥

–•—ç—Ä—ç–≤ –º—ç–¥—ç—ç–ª—ç–ª –¥—É—Ç—É—É –±–æ–ª —ç–µ–ª–¥–≥—ç—ç—Ä –∞—Å—É—É:
- "–Ø–º–∞—Ä —Ö–∏—á—ç—ç–ª–∏–π–Ω —Ç–µ—Å—Ç –±—ç–ª–¥—ç—Ö –≤—ç?"
- "–•—ç–¥–¥“Ø–≥—ç—ç—Ä –∞–Ω–≥–∏–π–Ω?"
- "–Ø–º–∞—Ä —Å—ç–¥–≤—ç—ç—Ä?"

–¢–µ—Å—Ç “Ø“Ø—Å–≥—ç—Ö–¥—ç—ç (—Ö—ç—Ä—ç–≥–ª—ç–≥—á ”©”©—Ä”©”©—Ä –∑–∞–∞–≥–∞–∞–≥“Ø–π –±–æ–ª):
- 10-15 –∞—Å—É—É–ª—Ç (—Å–æ–Ω–≥–æ—Ö, –Ω”©—Ö”©—Ö, –±–æ–¥–ª–æ–≥–æ —Ö–æ–ª–∏–º–æ–≥)
- 45 –º–∏–Ω—É—Ç
- –î—É–Ω–¥ —Ç“Ø–≤—à–∏–Ω
- –•–∞—Ä–∏—É–ª—Ç—ã–Ω —Ç“Ø–ª—Ö“Ø“Ø—Ä + —Ç–∞–π–ª–±–∞—Ä

–¢–µ—Å—Ç–∏–π–Ω —Ñ–æ—Ä–º–∞—Ç:

üìã [–ê–ù–ì–ò]-—Ä –∞–Ω–≥–∏ | [–•–ò–ß–≠–≠–õ] | [–°–≠–î–≠–í]
‚è± 45 –º–∏–Ω—É—Ç | 15 –∞—Å—É—É–ª—Ç

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

I. –°–û–ù–ì–û–• –•–ê–†–ò–£–õ–¢–¢–ê–ô (–∞—Å—É—É–ª—Ç –±“Ø—Ä 2 –æ–Ω–æ–æ)

1. [–ê—Å—É—É–ª—Ç]
   A) ...  B) ...  C) ...  D) ...

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

II. –ë–û–î–õ–û–ì–û (–∞—Å—É—É–ª—Ç –±“Ø—Ä 3-5 –æ–Ω–æ–æ)

1. [–ë–æ–¥–ª–æ–≥–æ]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìñ –•–ê–†–ò–£–õ–¢–´–ù –¢“Æ–õ–•“Æ“Æ–†

I. –°–æ–Ω–≥–æ—Ö: 1-B, 2-A, ...
II. –ë–æ–¥–ª–æ–≥–æ:
1. [–•–∞—Ä–∏—É–ª—Ç] - –¢–∞–π–ª–±–∞—Ä: ...
`;

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model: "openai" | "gemini";
  textbookContext?: string;
  imageUrl?: string;
}

export async function POST(request: NextRequest) {
  try {
    // Auth check
    const { userId, getToken } = await auth();
    if (!userId) {
      return new Response("Unauthorized", { status: 401 });
    }

    // Set Convex auth for personalization queries
    const token = await getToken({ template: "convex" });
    if (token) {
      convex.setAuth(token);
    }

    const body: ChatRequest = await request.json();
    const { messages, model, textbookContext, imageUrl } = body;

    if (!messages || !model) {
      return new Response("Missing messages or model", { status: 400 });
    }

    // Validate model
    if (model !== "openai" && model !== "gemini") {
      return new Response("Invalid model. Use 'openai' or 'gemini'.", {
        status: 400,
      });
    }

    // SSRF prevention: only accept data URLs for images
    if (imageUrl && !imageUrl.startsWith("data:image/")) {
      return new Response(
        "Invalid image URL format. Only data URLs are accepted.",
        { status: 400 },
      );
    }

    // Input validation
    if (messages.length > 50) {
      return new Response("Too many messages. Maximum 50 allowed.", {
        status: 400,
      });
    }

    if (messages.some((m) => m.content && m.content.length > 4000)) {
      return new Response(
        "Message content too long. Maximum 4000 characters per message.",
        { status: 400 },
      );
    }

    if (textbookContext && textbookContext.length > 10000) {
      return new Response(
        "Textbook context too long. Maximum 10000 characters.",
        { status: 400 },
      );
    }

    // Strip system role from client-provided messages (server adds its own)
    const sanitizedMessages = messages.filter((m) => m.role !== "system");

    // Fetch student's weak areas for personalization (non-blocking, fail-safe)
    let personalizationContext = "";
    try {
      if (token) {
        const weakTopics = await convex.query(api.topicMastery.getWeakTopics);
        if (weakTopics && weakTopics.length > 0) {
          const weakList = weakTopics
            .slice(0, 5)
            .map((t) => t.topicTitle)
            .join(", ");
          personalizationContext = `\n\n–°—É—Ä–∞–≥—á–∏–π–Ω —Å—É–ª —Ç–∞–ª—É—É–¥: ${weakList}. –≠–Ω—ç —Å—É—Ä–∞–≥—á–∏–¥ —Ö–∞—Ä–∏—É–ª–∞—Ö “Ø–µ–¥—ç—ç —Ç—ç–¥–Ω–∏–π —Å—É–ª —Ç–∞–ª—É—É–¥—ã–≥ –∞–Ω—Ö–∞–∞—Ä—á, –Ω—ç–º—ç–ª—Ç —Ç–∞–π–ª–±–∞—Ä ”©–≥”©—Ö.`;
        }
      }
    } catch {
      // Personalization is optional, don't block chat
    }

    const systemPrompt = textbookContext
      ? `${SYSTEM_PROMPT}${personalizationContext}\n\n–°—É—Ä–∞–≥—á –¥–∞—Ä–∞–∞—Ö —Å—É—Ä–∞—Ö –±–∏—á–≥–∏–π–Ω —Ö–∏—á—ç—ç–ª–∏–π–≥ –ª–∞–≤–ª–∞–∂ –±–∞–π–Ω–∞:\n${textbookContext}\n\n–≠–Ω—ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–¥ —Ç–æ—Ö–∏—Ä—É—É–ª–∞–Ω —Ö–∞—Ä–∏—É–ª–∞–∞—Ä–∞–π.`
      : `${SYSTEM_PROMPT}${personalizationContext}`;

    const messagesWithSystem: ChatMessage[] = [
      { role: "system", content: systemPrompt },
      ...sanitizedMessages,
    ];

    if (model === "openai") {
      return streamOpenAI(messagesWithSystem, imageUrl);
    } else if (model === "gemini") {
      return streamGemini(messagesWithSystem, imageUrl);
    } else {
      return new Response("Invalid model. Use 'openai' or 'gemini'.", {
        status: 400,
      });
    }
  } catch (error) {
    console.error("Chat API error:", error);
    return new Response("Internal server error", { status: 500 });
  }
}

async function streamOpenAI(
  messages: ChatMessage[],
  imageUrl?: string,
): Promise<Response> {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    return new Response("OPENAI_API_KEY not configured", { status: 500 });
  }

  const openai = new OpenAI({ apiKey });

  // Build messages, converting last user message to multimodal if image present
  type OpenAIMessage =
    | { role: "user" | "assistant" | "system"; content: string }
    | {
        role: "user";
        content: Array<
          | { type: "text"; text: string }
          | { type: "image_url"; image_url: { url: string; detail: "high" } }
        >;
      };

  const openaiMessages: OpenAIMessage[] = messages.map((m, i) => {
    if (imageUrl && m.role === "user" && i === messages.length - 1) {
      return {
        role: "user" as const,
        content: [
          {
            type: "text" as const,
            text:
              m.content ||
              "–≠–Ω—ç –∑—É—Ä–≥–∏–π–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂, –±–æ–¥–ª–æ–≥—ã–≥ –∞–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä –±–æ–¥–æ–∂ ”©–≥–Ω”© “Ø“Ø.",
          },
          {
            type: "image_url" as const,
            image_url: { url: imageUrl, detail: "high" as const },
          },
        ],
      };
    }
    return { role: m.role, content: m.content };
  });

  const stream = await openai.chat.completions.create({
    model: imageUrl ? "gpt-4o" : "gpt-4o-mini",
    messages: openaiMessages,
    stream: true,
    temperature: 0,
    max_tokens: 4096,
  });

  const encoder = new TextEncoder();

  const readable = new ReadableStream({
    async start(controller) {
      try {
        for await (const chunk of stream) {
          const content = chunk.choices[0]?.delta?.content;
          if (content) {
            controller.enqueue(encoder.encode(content));
          }
        }
        controller.close();
      } catch (error) {
        console.error("OpenAI stream error:", error);
        controller.error(error);
      }
    },
  });

  return new Response(readable, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}

function parseDataUrl(dataUrl: string): { mimeType: string; data: string } {
  // Parse data:image/png;base64,iVBOR... format
  const match = dataUrl.match(/^data:([^;]+);base64,(.+)$/);
  if (!match) {
    throw new Error("Invalid data URL format");
  }
  return { mimeType: match[1], data: match[2] };
}

async function streamGemini(
  messages: ChatMessage[],
  imageUrl?: string,
): Promise<Response> {
  const apiKey = process.env.GOOGLE_AI_API_KEY;
  if (!apiKey) {
    return new Response("GOOGLE_AI_API_KEY not configured", { status: 500 });
  }

  const genAI = new GoogleGenerativeAI(apiKey);
  const geminiModel = genAI.getGenerativeModel({
    model: "gemini-2.5-flash-lite",
  });

  // Convert messages to Gemini format
  // Gemini doesn't support "system" role in contents, so we use systemInstruction
  const systemMessage = messages.find((m) => m.role === "system");
  const chatMessages = messages.filter((m) => m.role !== "system");

  const contents = chatMessages.map((m, i) => {
    const parts: Array<
      { text: string } | { inlineData: { mimeType: string; data: string } }
    > = [];
    const text =
      m.content ||
      "–≠–Ω—ç –∑—É—Ä–≥–∏–π–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂, –±–æ–¥–ª–æ–≥—ã–≥ –∞–ª—Ö–∞–º –∞–ª—Ö–º–∞–∞—Ä –±–æ–¥–æ–∂ ”©–≥–Ω”© “Ø“Ø.";
    parts.push({ text });

    // Add image to the last user message
    if (imageUrl && m.role === "user" && i === chatMessages.length - 1) {
      const imageData = parseDataUrl(imageUrl);
      parts.push({ inlineData: imageData });
    }

    return {
      role: m.role === "assistant" ? "model" : "user",
      parts,
    };
  });

  const result = await geminiModel.generateContentStream({
    contents,
    systemInstruction: systemMessage
      ? { role: "user", parts: [{ text: systemMessage.content }] }
      : undefined,
  });

  const encoder = new TextEncoder();

  const readable = new ReadableStream({
    async start(controller) {
      try {
        for await (const chunk of result.stream) {
          const text = chunk.text();
          if (text) {
            controller.enqueue(encoder.encode(text));
          }
        }
        controller.close();
      } catch (error) {
        console.error("Gemini stream error:", error);
        controller.error(error);
      }
    },
  });

  return new Response(readable, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
